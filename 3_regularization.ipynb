{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Assignment 3\n",
    "\n",
    "Previously in 2_fullyconnected.ipynb, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First reload the data we generated in 1_notmnist.ipynb.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "\n",
    "    * data as a flat matrix,\n",
    "    * labels as float 1-hot encodings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor t using nn.l2_loss(t). The right amount of regularization should improve your validation / test accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 21.212198\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 12.9%\n",
      "Minibatch loss at step 500: 2.683192\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 1000: 1.384890\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 1500: 1.427944\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 2000: 0.963595\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 2500: 0.836461\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 3000: 0.804502\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 89.0%\n"
     ]
    }
   ],
   "source": [
    "## Logistic regression with regularization\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul2 = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "    beta_regul2* tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  #valid_prediction = tf.nn.softmax(\n",
    "  #  tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  #test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "  def calc_prediction(X):\n",
    "        outputLayer = tf.nn.softmax(tf.matmul(X, weights) + biases)\n",
    "        return outputLayer\n",
    "  test_prediction = calc_prediction(tf_test_dataset)\n",
    "  valid_prediction = calc_prediction(tf_valid_dataset)\n",
    "    \n",
    "    # now run the training\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul2: 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "accuracy on test has definitely improved from 85% to 89 %. However it introduces another hyper parameter \"beta_regul2\" that needs to be tuned on the validation set.\n",
    "Lets change the value of the param \"beta_regul2\" and see how the validation accuray changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3]\n"
     ]
    }
   ],
   "source": [
    "beta_list = [1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1]\n",
    "print(beta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " training for beta = 0.000100 \n",
      "initialized\n",
      " training for beta = 0.000300 \n",
      "initialized\n",
      " training for beta = 0.001000 \n",
      "initialized\n",
      " training for beta = 0.003000 \n",
      "initialized\n",
      " training for beta = 0.010000 \n",
      "initialized\n",
      " training for beta = 0.030000 \n",
      "initialized\n",
      " training for beta = 0.100000 \n",
      "initialized\n",
      " training for beta = 0.300000 \n",
      "initialized\n"
     ]
    }
   ],
   "source": [
    "# now run the training for each beta in the beta_list\n",
    "valid_accuracy =[]\n",
    "best_accuracy =0\n",
    "best_beta = beta_list[0]\n",
    "for beta in beta_list:\n",
    "    print(\" training for beta = %f \" %beta)\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"initialized\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # used a better randomization across epochs\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch\n",
    "            batch_data = train_dataset[offset:offset+batch_size, :]\n",
    "            batch_labels = train_labels[offset:offset+batch_size, :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, beta_regul2: beta}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
    "        # out of for loop- finished num_steps for current beta\n",
    "        curr_valid_accu = accuracy(valid_prediction.eval(), valid_labels)\n",
    "        valid_accuracy.append(curr_valid_accu)\n",
    "        if (curr_valid_accu > best_accuracy):\n",
    "                best_accuracy = curr_valid_accu\n",
    "                best_beta = beta\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best beta is 0.003000 with best validation accracy of 81.850000\n"
     ]
    }
   ],
   "source": [
    "# now plot the graph\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogx(beta_list, valid_accuracy)\n",
    "plt.title(\" plot of accuracy V regularization scaling param\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(\"The best beta is %f with best validation accracy of %f\" %(best_beta, best_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimum beta is 0.003; hence we need to retrain with this value to obtain the true test accuracy\n",
    "### Note: one cannot tune hyperparater to maximize the test accuracy! usign test dataet for hyperparamter tuning is cheating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized\n",
      "Minibatch loss at step 0: 24.661728\n",
      "Minibatch train accuracy 12.500000 \n",
      "Minibatch loss at step 500: 2.631405\n",
      "Minibatch train accuracy 78.125000 \n",
      "Minibatch loss at step 1000: 0.804276\n",
      "Minibatch train accuracy 85.156250 \n",
      "Minibatch loss at step 1500: 0.933653\n",
      "Minibatch train accuracy 73.437500 \n",
      "Minibatch loss at step 2000: 0.779794\n",
      "Minibatch train accuracy 80.468750 \n",
      "Minibatch loss at step 2500: 0.689051\n",
      "Minibatch train accuracy 85.156250 \n",
      "Minibatch loss at step 3000: 0.753786\n",
      "Minibatch train accuracy 78.906250 \n",
      " training completed\n",
      " final validation accuracy:  81.79\n",
      " final test accuracy:  88.74\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"initialized\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # used a better randomization across epochs\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch\n",
    "            batch_data = train_dataset[offset:offset+batch_size, :]\n",
    "            batch_labels = train_labels[offset:offset+batch_size, :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, beta_regul2: best_beta}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
    "            if (step%500 == 0):\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch train accuracy %f \" % accuracy(predictions, batch_labels))\n",
    "        print(\" training completed\")\n",
    "        print(\" final validation accuracy: \", accuracy(valid_prediction.eval(), valid_labels))\n",
    "        print(\" final test accuracy: \", accuracy(test_prediction.eval(), test_labels))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized 3 layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_nodes_hidden_layer1 = 1024\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  beta_regul2 = tf.placeholder(tf.float32)\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  #1st layer nodes = 784 =image_size*image_size\n",
    "  #2nd layer nodes(hidden layer) = 1024(=num_nodes_hidden_layer1)\n",
    "  #3rd layer nodes(output node, FC) = 10(= num_labels)\n",
    "  # hiddenout = relu(X*W1 + b1); the shape of weights and input X should allow matrix multiplication\n",
    "  # bias(b1), should be of shape [1, 1024]\n",
    "  # X:shape = [batch_size,784]\n",
    "  #define weights1 [784, 1024]\n",
    "  weights1 = tf.Variable(tf.truncated_normal(shape=[image_size*image_size, num_nodes_hidden_layer1]))\n",
    "  bias1 = tf.Variable(tf.zeros(shape =[1, num_nodes_hidden_layer1]))\n",
    "  \n",
    "  #hidden layer output = X*W1 +b1\n",
    "  hidden_out = tf.add(tf.matmul(tf_train_dataset, weights1), bias1)\n",
    "  hidden_out = tf.nn.relu(hidden_out)\n",
    "  # shape of hidden_out = [batch_size, 1024]\n",
    "  #now define the output layer as fully connected layer(FC)\n",
    "  weights2 = tf.Variable(tf.truncated_normal(shape=[num_nodes_hidden_layer1, num_labels]))\n",
    "\n",
    "  bias2 = tf.Variable(tf.zeros(shape=[1, num_labels]))\n",
    "  prediction_logit = tf.add(tf.matmul(hidden_out, weights2), bias2)\n",
    "  #prediction_out = tf.nn.softmax(prediction_logit)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= prediction_logit,labels= tf_train_labels)) + \\\n",
    "    beta_regul2*(tf.nn.l2_loss(weights1)+ tf.nn.l2_loss(weights2))\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "  #predictions for train, valid and test data\n",
    "  nntrain_prediction = tf.nn.softmax(prediction_logit)\n",
    "  #define  afunction that calculates prediction using the layer1 and layer2  learned parameters\n",
    "  def calc_prediction(X):\n",
    "        layer1 = tf.nn.relu(tf.matmul(X, weights1)+ bias1)\n",
    "        outputLayer = tf.nn.softmax(tf.matmul(layer1, weights2) + bias2)\n",
    "        return outputLayer\n",
    "  valid_prediction = calc_prediction(tf_valid_dataset)\n",
    "  test_prediction = calc_prediction(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 1202.828857\n",
      "Train Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 33.0%\n",
      "Minibatch loss at step 500: 520.317505\n",
      "Train Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 1000: 281.583069\n",
      "Train Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 1500: 154.296570\n",
      "Train Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 2000: 84.558937\n",
      "Train Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 2500: 46.317482\n",
      "Train Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 3000: 25.680408\n",
      "Train Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.1%\n",
      "Test accuracy: 93.1%\n"
     ]
    }
   ],
   "source": [
    "# now run the training\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul2:3e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, nntrain_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Train Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot the same graph; beta Vs accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " training for beta = 0.000100 \n",
      "initialized\n",
      " training for beta = 0.000300 \n",
      "initialized\n",
      " training for beta = 0.001000 \n",
      "initialized\n",
      " training for beta = 0.003000 \n",
      "initialized\n",
      " training for beta = 0.010000 \n",
      "initialized\n",
      " training for beta = 0.030000 \n",
      "initialized\n",
      " training for beta = 0.100000 \n",
      "initialized\n",
      " training for beta = 0.300000 \n",
      "initialized\n"
     ]
    }
   ],
   "source": [
    "# now run the training for each beta in the beta_list\n",
    "valid_accuracy =[]\n",
    "best_accuracy =0\n",
    "best_beta = beta_list[0]\n",
    "for beta in beta_list:\n",
    "    print(\" training for beta = %f \" %beta)\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"initialized\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # used a better randomization across epochs\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch\n",
    "            batch_data = train_dataset[offset:offset+batch_size, :]\n",
    "            batch_labels = train_labels[offset:offset+batch_size, :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, beta_regul2: beta}\n",
    "            _, l, predictions = session.run([optimizer, loss, nntrain_prediction], feed_dict = feed_dict)\n",
    "        # out of for loop- finished num_steps for current beta\n",
    "        curr_valid_accu = accuracy(valid_prediction.eval(), valid_labels)\n",
    "        valid_accuracy.append(curr_valid_accu)\n",
    "        if (curr_valid_accu > best_accuracy):\n",
    "                best_accuracy = curr_valid_accu\n",
    "                best_beta = beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best beta is 0.003000 with best validation accracy of 87.140000\n"
     ]
    }
   ],
   "source": [
    "# now plot the graph\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogx(beta_list, valid_accuracy)\n",
    "plt.title(\" plot of accuracy V regularization param (3 layer NN)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(\"The best beta is %f with best validation accracy of %f\" %(best_beta, best_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will attempt this problem by running the training for a percentage of batch size(25%) instead of complete batch size(128). To make this more visible clear, we will introduce a new loop for no_of_epochs and run through the SGD batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes_hidden_layer1 = 1024\n",
    "batch_size = 128\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  beta_regul2 = tf.placeholder(tf.float32)\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  #1st layer nodes = 784 =image_size*image_size\n",
    "  #2nd layer nodes(hidden layer) = 1024(=num_nodes_hidden_layer1)\n",
    "  #3rd layer nodes(output node, FC) = 10(= num_labels)\n",
    "  # hiddenout = relu(X*W1 + b1); the shape of weights and input X should allow matrix multiplication\n",
    "  # bias(b1), should be of shape [1, 1024]\n",
    "  # X:shape = [batch_size,784]\n",
    "  #define weights1 [784, 1024]\n",
    "  weights1 = tf.Variable(tf.truncated_normal(shape=[image_size*image_size, num_nodes_hidden_layer1]))\n",
    "  bias1 = tf.Variable(tf.zeros(shape =[1, num_nodes_hidden_layer1]))\n",
    "  \n",
    "  #hidden layer output = X*W1 +b1\n",
    "  hidden_out = tf.add(tf.matmul(tf_train_dataset, weights1), bias1)\n",
    "  hidden_out = tf.nn.relu(hidden_out)\n",
    "  # shape of hidden_out = [batch_size, 1024]\n",
    "  #now define the output layer as fully connected layer(FC)\n",
    "  weights2 = tf.Variable(tf.truncated_normal(shape=[num_nodes_hidden_layer1, num_labels]))\n",
    "\n",
    "  bias2 = tf.Variable(tf.zeros(shape=[1, num_labels]))\n",
    "  prediction_logit = tf.add(tf.matmul(hidden_out, weights2), bias2)\n",
    "  #prediction_out = tf.nn.softmax(prediction_logit)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= prediction_logit,labels= tf_train_labels)) + \\\n",
    "    beta_regul2*(tf.nn.l2_loss(weights1)+ tf.nn.l2_loss(weights2))\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "  #predictions for train, valid and test data\n",
    "  nntrain_prediction = tf.nn.softmax(prediction_logit)\n",
    "  #define  afunction that calculates prediction using the layer1 and layer2  learned parameters\n",
    "  def calc_prediction(X):\n",
    "        layer1 = tf.nn.relu(tf.matmul(X, weights1)+ bias1)\n",
    "        outputLayer = tf.nn.softmax(tf.matmul(layer1, weights2) + bias2)\n",
    "        return outputLayer\n",
    "  valid_prediction = calc_prediction(tf_valid_dataset)\n",
    "  test_prediction = calc_prediction(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " global variables initialization complete \n",
      "********epoch no 0********* \n",
      "At step = 1, minibatch loss =1298.718262 \n",
      "minibatch train accuracy =  6.25\n",
      "At step = 501, minibatch loss =522.078674 \n",
      "minibatch train accuracy =  81.25\n",
      "At step = 1001, minibatch loss =281.130310 \n",
      "minibatch train accuracy =  87.5\n",
      "At step = 1501, minibatch loss =154.259048 \n",
      "minibatch train accuracy =  80.46875\n",
      "After epoch 0, validation accuracy = 83.350000\n",
      "Test accuracy = 90.100000\n",
      "********epoch no 1********* \n",
      "At step = 2, minibatch loss =142.761124 \n",
      "minibatch train accuracy =  84.375\n",
      "At step = 502, minibatch loss =105.884590 \n",
      "minibatch train accuracy =  82.8125\n",
      "At step = 1002, minibatch loss =78.136848 \n",
      "minibatch train accuracy =  86.71875\n",
      "At step = 1502, minibatch loss =58.089394 \n",
      "minibatch train accuracy =  84.375\n",
      "At step = 2002, minibatch loss =42.866852 \n",
      "minibatch train accuracy =  93.75\n",
      "At step = 2502, minibatch loss =31.987947 \n",
      "minibatch train accuracy =  88.28125\n",
      "At step = 3002, minibatch loss =23.874964 \n",
      "minibatch train accuracy =  85.15625\n",
      "After epoch 1, validation accuracy = 87.420000\n",
      "Test accuracy = 93.350000\n",
      "********epoch no 2********* \n",
      "At step = 3, minibatch loss =22.119936 \n",
      "minibatch train accuracy =  89.0625\n",
      "At step = 1503, minibatch loss =12.326000 \n",
      "minibatch train accuracy =  89.84375\n",
      "At step = 3003, minibatch loss =6.913088 \n",
      "minibatch train accuracy =  94.53125\n",
      "At step = 4503, minibatch loss =4.206469 \n",
      "minibatch train accuracy =  85.15625\n",
      "After epoch 2, validation accuracy = 87.950000\n",
      "Test accuracy = 93.540000\n",
      " Final Test accuracy = 93.540000\n"
     ]
    }
   ],
   "source": [
    "# now training\n",
    "# define epochs = no of times to run through the data\n",
    "epochs = 3\n",
    "\n",
    "with tf.Session(graph= graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\" global variables initialization complete \")\n",
    "    # num_batches = no of steps required to complete one pass of train_dataset\n",
    "    num_batches = int(len(train_dataset)/batch_size)\n",
    "    for i in range(epochs):\n",
    "        print(\"********epoch no %d********* \" %i)\n",
    "        for n in range(num_batches):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            offset = (n * batch_size) # % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:offset+batch_size, :]\n",
    "            batch_labels = train_labels[offset:offset+batch_size, :]\n",
    "            feed_dict = {tf_train_dataset:batch_data, tf_train_labels:batch_labels, beta_regul2:3e-3 }\n",
    "            # making beta_regu=0 as we want to introduce overfitting\n",
    "            _,l, prediction = session.run([optimizer, loss,nntrain_prediction], feed_dict = feed_dict)\n",
    "            if(((i+1)*n) % 500 == 0):\n",
    "                print(\"At step = %d, minibatch loss =%f \"%((n+1)*(i+1), l))\n",
    "                print(\"minibatch train accuracy = \", accuracy(prediction, batch_labels))\n",
    "        print(\"After epoch %d, validation accuracy = %f\" %(i, accuracy(valid_prediction.eval(), valid_labels)) )\n",
    "        print(\"Test accuracy = %f\" % accuracy(test_prediction.eval(), test_labels) )\n",
    "    print(\" Final Test accuracy = %f\" % accuracy(test_prediction.eval(), test_labels) )\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For problem 2 we will modify the above training and feed the overlapping sequence of training data(similar). \n",
    "Since the model will see almost same data for training, training accuracy will be ~100% but low generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " global variables initialization complete \n",
      "********epoch no 0********* \n",
      "At step = 1, minibatch loss =332.773010 \n",
      "minibatch train accuracy =  10.15625\n",
      "At step = 501, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "At step = 1001, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "At step = 1501, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "After epoch 0, validation accuracy = 60.950000\n",
      "Test accuracy = 68.200000\n",
      "********epoch no 1********* \n",
      "At step = 2, minibatch loss =135.155670 \n",
      "minibatch train accuracy =  59.375\n",
      "At step = 502, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "At step = 1002, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "At step = 1502, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "At step = 2002, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "At step = 2502, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "At step = 3002, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "After epoch 1, validation accuracy = 70.510000\n",
      "Test accuracy = 78.170000\n",
      "********epoch no 2********* \n",
      "At step = 3, minibatch loss =88.925003 \n",
      "minibatch train accuracy =  71.875\n",
      "At step = 1503, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "At step = 3003, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "At step = 4503, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "After epoch 2, validation accuracy = 72.770000\n",
      "Test accuracy = 80.400000\n",
      " Final Test accuracy = 80.400000\n"
     ]
    }
   ],
   "source": [
    "# now training\n",
    "# define epochs = no of times to run through the data\n",
    "epochs = 3\n",
    "\n",
    "with tf.Session(graph= graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\" global variables initialization complete \")\n",
    "    # num_batches = no of steps required to complete one pass of train_dataset\n",
    "    num_batches = int(len(train_dataset)/batch_size)\n",
    "    for i in range(epochs):\n",
    "        print(\"********epoch no %d********* \" %i)\n",
    "        for n in range(num_batches):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            #offset = (n * batch_size) # \n",
    "            offset = int(i * batch_size)# we will train on the same dataset multiple times\n",
    "            batch_data = train_dataset[offset:offset+batch_size, :]\n",
    "            batch_labels = train_labels[offset:offset+batch_size, :]\n",
    "            feed_dict = {tf_train_dataset:batch_data, tf_train_labels:batch_labels, beta_regul2:0 }\n",
    "            # make beta_regu=0 if we want to introduce more overfitting\n",
    "            _,l, prediction = session.run([optimizer, loss,nntrain_prediction], feed_dict = feed_dict)\n",
    "            if(((i+1)*n) % 500 == 0):\n",
    "                print(\"At step = %d, minibatch loss =%f \"%((n+1)*(i+1), l))\n",
    "                print(\"minibatch train accuracy = \", accuracy(prediction, batch_labels))\n",
    "        print(\"After epoch %d, validation accuracy = %f\" %(i, accuracy(valid_prediction.eval(), valid_labels)) )\n",
    "        print(\"Test accuracy = %f\" % accuracy(test_prediction.eval(), test_labels) )\n",
    "    print(\" Final Test accuracy = %f\" % accuracy(test_prediction.eval(), test_labels) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different offset such as\n",
    "- 1) offset = int((n% epochs) * batch_size)\n",
    "- 2) offset = int(n * batch_size/32)\n",
    "\n",
    "Even regularization will not improve the  train accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides nn.dropout() for that, but you have to make sure it's only inserted during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes_hidden_layer1 = 1024\n",
    "batch_size = 128\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  beta_regul2 = tf.placeholder(tf.float32)\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  #1st layer nodes = 784 =image_size*image_size\n",
    "  #2nd layer nodes(hidden layer) = 1024(=num_nodes_hidden_layer1)\n",
    "  #3rd layer nodes(output node, FC) = 10(= num_labels)\n",
    "  # hiddenout = relu(X*W1 + b1); the shape of weights and input X should allow matrix multiplication\n",
    "  # bias(b1), should be of shape [1, 1024]\n",
    "  # X:shape = [batch_size,784]\n",
    "  #define weights1 [784, 1024]\n",
    "  weights1 = tf.Variable(tf.truncated_normal(shape=[image_size*image_size, num_nodes_hidden_layer1]))\n",
    "  bias1 = tf.Variable(tf.zeros(shape =[1, num_nodes_hidden_layer1]))\n",
    "  \n",
    "  #hidden layer output = X*W1 +b1\n",
    "  hidden_lay1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1)+ bias1)\n",
    "  hidden_out = tf.nn.dropout(hidden_lay1, 0.5)# dropout added to training\n",
    "  # shape of hidden_out = [batch_size, 1024]\n",
    "  #now define the output layer as fully connected layer(FC)\n",
    "  weights2 = tf.Variable(tf.truncated_normal(shape=[num_nodes_hidden_layer1, num_labels]))\n",
    "  bias2 = tf.Variable(tf.zeros(shape=[1, num_labels]))\n",
    "  prediction_logit = tf.add(tf.matmul(hidden_out, weights2), bias2)\n",
    "  #prediction_out = tf.nn.softmax(prediction_logit)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= prediction_logit,labels= tf_train_labels)) + \\\n",
    "  beta_regul2*(tf.nn.l2_loss(weights1)+ tf.nn.l2_loss(weights2))\n",
    "  # we will make beta_regu =0 to make it non-regularized\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "  #predictions for train, valid and test data\n",
    "  nntrain_prediction = tf.nn.softmax(prediction_logit)\n",
    "  #define  afunction that calculates prediction using the layer1 and layer2  learned parameters\n",
    "    # note this function does not use dropout ( dropout is only for training)\n",
    "  def calc_prediction(X):\n",
    "        layer1 = tf.nn.relu(tf.matmul(X, weights1)+ bias1)\n",
    "        outputLayer = tf.nn.softmax(tf.matmul(layer1, weights2) + bias2)\n",
    "        return outputLayer\n",
    "  valid_prediction = calc_prediction(tf_valid_dataset)\n",
    "  test_prediction = calc_prediction(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " global variables initialization complete \n",
      "********epoch no 0********* \n",
      "At step = 1, minibatch loss =505.557404 \n",
      "minibatch train accuracy =  11.71875\n",
      "At step = 501, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "At step = 1001, minibatch loss =0.085312 \n",
      "minibatch train accuracy =  99.21875\n",
      "At step = 1501, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "After epoch 0, validation accuracy = 68.450000\n",
      "Test accuracy = 75.250000\n",
      "********epoch no 1********* \n",
      "At step = 2, minibatch loss =145.655945 \n",
      "minibatch train accuracy =  66.40625\n",
      "At step = 502, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "At step = 1002, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "At step = 1502, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "At step = 2002, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "At step = 2502, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "At step = 3002, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "After epoch 1, validation accuracy = 76.280000\n",
      "Test accuracy = 83.530000\n",
      "********epoch no 2********* \n",
      "At step = 3, minibatch loss =109.089027 \n",
      "minibatch train accuracy =  81.25\n",
      "At step = 1503, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "At step = 3003, minibatch loss =0.000000 \n",
      "minibatch train accuracy =  100.0\n",
      "At step = 4503, minibatch loss =0.000159 \n",
      "minibatch train accuracy =  100.0\n",
      "After epoch 2, validation accuracy = 76.760000\n",
      "Test accuracy = 84.250000\n",
      " Final Test accuracy = 84.250000\n"
     ]
    }
   ],
   "source": [
    "# now training\n",
    "# define epochs = no of times to run through the data\n",
    "epochs = 3\n",
    "\n",
    "with tf.Session(graph= graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\" global variables initialization complete \")\n",
    "    # num_batches = no of steps required to complete one pass of train_dataset\n",
    "    num_batches = int(len(train_dataset)/batch_size)\n",
    "    for i in range(epochs):\n",
    "        print(\"********epoch no %d********* \" %i)\n",
    "        for n in range(num_batches):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            #offset = (n * batch_size) # \n",
    "            offset = int(i * batch_size)# we will train on the same dataset multiple times\n",
    "            batch_data = train_dataset[offset:offset+batch_size, :]\n",
    "            batch_labels = train_labels[offset:offset+batch_size, :]\n",
    "            feed_dict = {tf_train_dataset:batch_data, tf_train_labels:batch_labels, beta_regul2:0 }\n",
    "            # make beta_regu=0 if we want to introduce more overfitting\n",
    "            _,l, prediction = session.run([optimizer, loss,nntrain_prediction], feed_dict = feed_dict)\n",
    "            if(((i+1)*n) % 500 == 0):\n",
    "                print(\"At step = %d, minibatch loss =%f \"%((n+1)*(i+1), l))\n",
    "                print(\"minibatch train accuracy = \", accuracy(prediction, batch_labels))\n",
    "        print(\"After epoch %d, validation accuracy = %f\" %(i, accuracy(valid_prediction.eval(), valid_labels)) )\n",
    "        print(\"Test accuracy = %f\" % accuracy(test_prediction.eval(), test_labels) )\n",
    "    print(\" Final Test accuracy = %f\" % accuracy(test_prediction.eval(), test_labels) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion: problem3\n",
    "- The test accuracy incresed slightly from 80% to 84%\n",
    "- This is extreme overfitting hence ther is not visible difference.\n",
    "- Note only 3*128 no of dataset are fed for training(This is extreme overfitting)\n",
    "- The usefulness of dropout would be relevant in case of very deep layer network( i guess)\n",
    "- Tenchnically, dropout does same job as L2 regularization, i.e. preventing overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is 97.1%.\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "- we will attempt to use 4 layer NN(2 hidden),L2 regularization, dropout, and learning rate decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes_hidden_layer1 = 1024\n",
    "num_nodes_hidden_layer2 = 128 # keeping multiple of 2^N\n",
    "batch_size = 128\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  beta_regul2 = tf.placeholder(tf.float32)\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  keep_prob = tf.placeholder(tf.float32)\n",
    "  #1st layer nodes = 784 =image_size*image_size\n",
    "  #2nd layer nodes(hidden layer1) = 1024(=num_nodes_hidden_layer1)\n",
    "  # 3rd layer node(hidden layer2) = 128\n",
    "  #4th layer nodes(output node, FC) = 10(= num_labels)\n",
    " \n",
    "  weights1 = tf.Variable(tf.random_normal(shape=[image_size*image_size, num_nodes_hidden_layer1],  stddev = 0.01))\n",
    "  bias1 = tf.Variable(tf.zeros(shape =[1, num_nodes_hidden_layer1]))\n",
    "  \n",
    "  weights2 = tf.Variable(tf.random_normal(shape=[num_nodes_hidden_layer1, num_nodes_hidden_layer2], stddev =0.01))\n",
    "  bias2 = tf.Variable(tf.zeros(shape=[1, num_nodes_hidden_layer2]))\n",
    "  \n",
    "  weights3 = tf.Variable(tf.random_normal(shape=[num_nodes_hidden_layer2, num_labels], stddev =0.01))\n",
    "  bias3 = tf.Variable(tf.zeros(shape=[1, num_labels]))\n",
    "\n",
    "  #hidden layer output = X*W1 +b1\n",
    "  hidden_lay1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1)+ bias1)\n",
    "  lay1_out = tf.nn.dropout(hidden_lay1, keep_prob)# dropout added to training\n",
    "  \n",
    "  hidden_lay2 = tf.nn.relu(tf.matmul(lay1_out, weights2)+ bias2)\n",
    "  lay2_out = tf.nn.dropout(hidden_lay2, keep_prob)\n",
    "  \n",
    "  prediction_logit = tf.add(tf.matmul(lay2_out, weights3), bias3)\n",
    "  #prediction_out = tf.nn.softmax(prediction_logit)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= prediction_logit,labels= tf_train_labels)) + \\\n",
    "  beta_regul2*(tf.nn.l2_loss(weights1)+ tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  # we can make beta_regu =0 to make it non-regularized\n",
    "  \n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(0.2, global_step,1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "  #predictions for train, valid and test data\n",
    "  nntrain_prediction = tf.nn.softmax(prediction_logit)\n",
    "  #define  afunction that calculates prediction using the layer1 and layer2  learned parameters\n",
    "    # note this function does not use dropout ( dropout is only for training)\n",
    "  def calc_prediction(X):\n",
    "        layer1 = tf.nn.relu(tf.matmul(X, weights1)+ bias1)\n",
    "        layer2 = tf.nn.relu(tf.matmul(layer1, weights2)+ bias2)\n",
    "        outputLayer = tf.nn.softmax(tf.matmul(layer2, weights3) + bias3)\n",
    "        return outputLayer\n",
    "  valid_prediction = calc_prediction(tf_valid_dataset)\n",
    "  test_prediction = calc_prediction(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " global variables initialization complete \n",
      "********epoch no 0********* \n",
      "At step = 1, minibatch loss =2.349010 \n",
      "minibatch train accuracy =  14.84375\n",
      "At step = 501, minibatch loss =0.542205 \n",
      "minibatch train accuracy =  85.9375\n",
      "At step = 1001, minibatch loss =0.376830 \n",
      "minibatch train accuracy =  92.1875\n",
      "At step = 1501, minibatch loss =0.608721 \n",
      "minibatch train accuracy =  83.59375\n",
      "After epoch 0, validation accuracy = 86.890000\n",
      "Test accuracy = 93.120000\n",
      "********epoch no 1********* \n",
      "At step = 2, minibatch loss =0.492205 \n",
      "minibatch train accuracy =  90.625\n",
      "At step = 502, minibatch loss =0.466378 \n",
      "minibatch train accuracy =  88.28125\n",
      "At step = 1002, minibatch loss =0.409575 \n",
      "minibatch train accuracy =  90.625\n",
      "At step = 1502, minibatch loss =0.500743 \n",
      "minibatch train accuracy =  86.71875\n",
      "At step = 2002, minibatch loss =0.329183 \n",
      "minibatch train accuracy =  93.75\n",
      "At step = 2502, minibatch loss =0.515460 \n",
      "minibatch train accuracy =  86.71875\n",
      "At step = 3002, minibatch loss =0.531918 \n",
      "minibatch train accuracy =  85.15625\n",
      "After epoch 1, validation accuracy = 88.440000\n",
      "Test accuracy = 94.230000\n",
      "********epoch no 2********* \n",
      "At step = 3, minibatch loss =0.446483 \n",
      "minibatch train accuracy =  91.40625\n",
      "At step = 1503, minibatch loss =0.387216 \n",
      "minibatch train accuracy =  90.625\n",
      "At step = 3003, minibatch loss =0.309346 \n",
      "minibatch train accuracy =  95.3125\n",
      "At step = 4503, minibatch loss =0.501358 \n",
      "minibatch train accuracy =  85.9375\n",
      "After epoch 2, validation accuracy = 89.000000\n",
      "Test accuracy = 94.590000\n",
      "********epoch no 3********* \n",
      "At step = 4, minibatch loss =0.431618 \n",
      "minibatch train accuracy =  92.1875\n",
      "At step = 504, minibatch loss =0.497421 \n",
      "minibatch train accuracy =  85.9375\n",
      "At step = 1004, minibatch loss =0.392670 \n",
      "minibatch train accuracy =  91.40625\n",
      "At step = 1504, minibatch loss =0.401978 \n",
      "minibatch train accuracy =  89.84375\n",
      "At step = 2004, minibatch loss =0.373375 \n",
      "minibatch train accuracy =  90.625\n",
      "At step = 2504, minibatch loss =0.463017 \n",
      "minibatch train accuracy =  89.0625\n",
      "At step = 3004, minibatch loss =0.446526 \n",
      "minibatch train accuracy =  89.0625\n",
      "At step = 3504, minibatch loss =0.412428 \n",
      "minibatch train accuracy =  89.84375\n",
      "At step = 4004, minibatch loss =0.302884 \n",
      "minibatch train accuracy =  95.3125\n",
      "At step = 4504, minibatch loss =0.461403 \n",
      "minibatch train accuracy =  88.28125\n",
      "At step = 5004, minibatch loss =0.481323 \n",
      "minibatch train accuracy =  88.28125\n",
      "At step = 5504, minibatch loss =0.439580 \n",
      "minibatch train accuracy =  91.40625\n",
      "At step = 6004, minibatch loss =0.485427 \n",
      "minibatch train accuracy =  86.71875\n",
      "After epoch 3, validation accuracy = 89.360000\n",
      "Test accuracy = 94.740000\n",
      "********epoch no 4********* \n",
      "At step = 5, minibatch loss =0.418801 \n",
      "minibatch train accuracy =  92.1875\n",
      "At step = 505, minibatch loss =0.420697 \n",
      "minibatch train accuracy =  89.84375\n",
      "At step = 1005, minibatch loss =0.444608 \n",
      "minibatch train accuracy =  87.5\n",
      "At step = 1505, minibatch loss =0.410176 \n",
      "minibatch train accuracy =  89.84375\n",
      "At step = 2005, minibatch loss =0.421884 \n",
      "minibatch train accuracy =  85.9375\n",
      "At step = 2505, minibatch loss =0.368407 \n",
      "minibatch train accuracy =  92.1875\n",
      "At step = 3005, minibatch loss =0.363910 \n",
      "minibatch train accuracy =  90.625\n",
      "At step = 3505, minibatch loss =0.450166 \n",
      "minibatch train accuracy =  89.0625\n",
      "At step = 4005, minibatch loss =0.367060 \n",
      "minibatch train accuracy =  92.1875\n",
      "At step = 4505, minibatch loss =0.474842 \n",
      "minibatch train accuracy =  89.84375\n",
      "At step = 5005, minibatch loss =0.298422 \n",
      "minibatch train accuracy =  94.53125\n",
      "At step = 5505, minibatch loss =0.455796 \n",
      "minibatch train accuracy =  89.0625\n",
      "At step = 6005, minibatch loss =0.421361 \n",
      "minibatch train accuracy =  89.0625\n",
      "At step = 6505, minibatch loss =0.369334 \n",
      "minibatch train accuracy =  92.96875\n",
      "At step = 7005, minibatch loss =0.465117 \n",
      "minibatch train accuracy =  90.625\n",
      "At step = 7505, minibatch loss =0.478817 \n",
      "minibatch train accuracy =  86.71875\n",
      "After epoch 4, validation accuracy = 89.370000\n",
      "Test accuracy = 94.830000\n",
      " Final Test accuracy = 94.830000\n"
     ]
    }
   ],
   "source": [
    "# now training\n",
    "# define epochs = no of times to run through the data\n",
    "epochs = 5\n",
    "\n",
    "with tf.Session(graph= graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\" global variables initialization complete \")\n",
    "    # num_batches = no of steps required to complete one pass of train_dataset\n",
    "    num_batches = int(len(train_dataset)/batch_size)\n",
    "    for i in range(epochs):\n",
    "        print(\"********epoch no %d********* \" %i)\n",
    "        for n in range(num_batches):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            offset = (n * batch_size) # \n",
    "            #offset = int(i * batch_size)# we will train on the same dataset multiple times\n",
    "            batch_data = train_dataset[offset:offset+batch_size, :]\n",
    "            batch_labels = train_labels[offset:offset+batch_size, :]\n",
    "            feed_dict = {tf_train_dataset:batch_data, tf_train_labels:batch_labels, beta_regul2:1e-3, keep_prob: 1 }\n",
    "            # make beta_regu=0 if we want to introduce more overfitting\n",
    "            _,l, prediction = session.run([optimizer, loss,nntrain_prediction], feed_dict = feed_dict)\n",
    "            if(((i+1)*n) % 500 == 0):\n",
    "                print(\"At step = %d, minibatch loss =%f \"%((n+1)*(i+1), l))\n",
    "                print(\"minibatch train accuracy = \", accuracy(prediction, batch_labels))\n",
    "        print(\"After epoch %d, validation accuracy = %f\" %(i, accuracy(valid_prediction.eval(), valid_labels)) )\n",
    "        print(\"Test accuracy = %f\" % accuracy(test_prediction.eval(), test_labels) )\n",
    "    print(\" Final Test accuracy = %f\" % accuracy(test_prediction.eval(), test_labels) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats my best score(94.8%)\n",
    "For some reason, disbaling dropout gives better performance in my case;\n",
    "I can probably try to increase the accuracy to 96% by including additional layer, but will not try it.\n",
    "Hopefully, using CNN in next lesson will improve accuracy drastically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
