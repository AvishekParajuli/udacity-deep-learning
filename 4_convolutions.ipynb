{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Deep Learning\n",
    "## Assignment 4\n",
    "\n",
    "Previously in 2_fullyconnected.ipynb and 3_regularization.ipynb, we trained fully connected networks to classify notMNIST characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Reformat into a TensorFlow-friendly shape:\n",
    "\n",
    "   - convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "   - labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-3b42a02b2811>:45: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.195922\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 10.4%\n",
      "Minibatch loss at step 500: 0.860197\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 1000: 0.128282\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 83.3%\n",
      "Test accuracy: 90.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion:\n",
    "This is a simple NN with two layers of Conv, followed by two FC layers.\n",
    "The architecture is: \n",
    "\n",
    "Input(N,28, 28,1)->conv(5,5,16, s=2)->ReLu->conv(5,5,16, s=2)->ReLu->FC(7*7*16)->FC->Out\n",
    "\n",
    "- The code is a bit unclear and there is no separation of Conv and Fully connectedlayer; \n",
    "- with batch_size=16, and num_steps = 1001; we are not using the whole training dataset of length= 200000.\n",
    "- Required steps to traverse the whole dataset (total no of batches)= 12500\n",
    "\n",
    "Next we will rewrite the above code in a easy-readable format and use it in next subsection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "batch_size = 16\n",
    "filter_size = 5\n",
    "in_image_channnels = 1 #grayscale image\n",
    "depth = 16 #no of channels in conv layer\n",
    "num_hidden = 64\n",
    "learning_rate = 0.05\n",
    "\n",
    "\"\"\"\n",
    "tensorflow conv2d requires 4D input and filter tensor;\n",
    "tf.nn.conv2d(\n",
    "    input,\n",
    "    filter,\n",
    "    strides,\n",
    "    padding,...)\n",
    "input tensor of shape = [batch, in_height, in_width, in_channels]\n",
    "filter / kernel tensor of shape = [filter_height, filter_width, in_channels, out_channels]\n",
    "\"\"\"\n",
    "\n",
    "#define a function that creates convolutional layer\n",
    "def create_conv_layer(in_data, num_in_channels, num_out_channels, filter_shape,conv_stride, name):\n",
    "    ''' in_data = input Data (need 4D shape defined above)\n",
    "        in_channels = no of channels in input image, 1(grayscale), 3(RGB)\n",
    "        out_channels = depth of conv layers\n",
    "        filter_shape = filter shape used for convolution, for e.g. [3,3], or [5,5]\n",
    "        name = any valid string'''\n",
    "    #define 4D shape for filter/kernel tensor that will be used for creating weights\n",
    "    conv_filter_shape = [filter_shape[0], filter_shape[1], num_in_channels, num_out_channels]\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal(conv_filter_shape, stddev = 0.1), name = name+ '_W')\n",
    "    bias = tf.Variable(tf.zeros(num_out_channels), name = name+\"_b\")\n",
    "    \n",
    "    strides = [1, conv_stride[0], conv_stride[1], 1]\n",
    "    #define the conv2d layer\n",
    "    outlayer = tf.nn.conv2d(in_data, weights, strides = strides, padding = 'SAME')\n",
    "    #add bias\n",
    "    outlayer = outlayer + bias\n",
    "    #apply ReLu activation\n",
    "    outlayer = tf.nn.relu(outlayer)\n",
    "    \n",
    "    #usually there is pooling layer; not in this case\n",
    "    return outlayer\n",
    "\n",
    "# the dataset train_dataset, train_labels is already formatted\n",
    "# Input data placeholders\n",
    "# using shape = [None, ] allows us to use it as a general label placeholder\n",
    "# shape=(batch_size, image_size, image_size, in_image_channnels)\n",
    "tf_train_dataset = tf.placeholder(tf.float32, shape=(None, image_size, image_size, in_image_channnels))\n",
    "#tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "tf_valid_dataset = tf.constant(valid_dataset)\n",
    "tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "# MODEL definition\n",
    "# now define the two conv layer\n",
    "layer1 = create_conv_layer(tf_train_dataset, in_image_channnels, depth,\\\n",
    "                           filter_shape =[5,5],conv_stride =[2,2], name ='layer1')\n",
    "layer2 = create_conv_layer(layer1,depth, 16, [5,5], [2,2], name ='layer2' )\n",
    "\n",
    "# now we need to flatten and add two FC layers\n",
    "# after two layers of stride 2, we go from 28 x 28, to 14 x 14 to 7 x 7 x,y co-ordinates, \n",
    "# but with 16 output channels.  To create the fully connected,\n",
    "# \"dense\" layer, the new shape needs to be [-1, 7 x 7 x 16]\n",
    "#flattened = tf.reshape(layer2, [-1, 7 * 7 * 16])\n",
    "flattened = tf.reshape(layer2, [-1, 7 * 7 * 16])\n",
    "\n",
    "#setup weights and bias for dense layer\n",
    "w1 = tf.Variable(tf.truncated_normal([7*7* 16, num_hidden], stddev = 0.1), name ='w_dense1')\n",
    "b1 = tf.Variable(tf.zeros([num_hidden]), name ='b_dense1')\n",
    "# fully connected layer1\n",
    "dense_layer1 = tf.nn.relu(tf.matmul(flattened, w1) + b1)\n",
    "\n",
    "# weights and bias for dense layer2\n",
    "w2 = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev = 0.1), name ='w_dense2')\n",
    "b2 = tf.Variable(tf.zeros([num_labels]), name = 'b_dense2')\n",
    "# define the output logits = dense_layer\n",
    "dense_layer2 = tf.matmul(dense_layer1, w2) + b2\n",
    "\n",
    "# get the logits from the model\n",
    "logits = dense_layer2\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "# Optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.nn.softmax(logits)\n",
    "# define an accuracy assessment operation\n",
    "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(tf_train_labels, 1))\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Predictions for the training, validation, and test data.\n",
    "# these predictions are derrived by \n",
    "# calling the accuracy_op operation and feeding the placeholders in feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      " Total training dataset length:  200000\n",
      " total no of batches:  12500\n",
      "Epoch: 1 cost = 0.748  test accuracy: 94.830%\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# now training part\n",
    "epochs = 1\n",
    "\n",
    "with tf.Session() as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  total_batches = int(train_labels.shape[0]/batch_size)\n",
    "  #total_batches = 1001 # = num_Steps\n",
    "  print(\" Total training dataset length: \", len(train_labels))\n",
    "  print(\" total no of batches: \", total_batches)\n",
    "  for epoch in range(epochs):\n",
    "      for i in range(total_batches):\n",
    "          offset = (i * batch_size)\n",
    "          batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "          batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "          feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "          _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "      test_acc = session.run(accuracy_op, \\\n",
    "                             feed_dict ={tf_train_dataset:test_dataset,tf_train_labels: test_labels})\n",
    "      print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(l), \" test accuracy: {:.3f}%\".format(test_acc*100))\n",
    "  print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOW! with just 1 epoch and running through the complete train_dataset, we achieved accuracy of 94.8%.(last one =90%)\n",
    "\n",
    "Most times, people try to increase layers in the model rather than getting a picture of whats happening. Here even with the same model, we achived better accuracy after making 100% use of data.\n",
    "\n",
    "Lets run through the training dataset thrice(epochs= 3) and lets examine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      " Total training dataset length:  200000\n",
      " total no of batches:  12500\n",
      "Epoch: 1 cost = 0.690  test accuracy: 94.640%\n",
      "Epoch: 2 cost = 0.695  test accuracy: 95.120%\n",
      "Epoch: 3 cost = 0.669  test accuracy: 95.470%\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# now training part\n",
    "epochs = 3\n",
    "\n",
    "with tf.Session() as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  total_batches = int(train_labels.shape[0]/batch_size)\n",
    "  #total_batches = 1001 # = num_Steps\n",
    "  print(\" Total training dataset length: \", len(train_labels))\n",
    "  print(\" total no of batches: \", total_batches)\n",
    "  for epoch in range(epochs):\n",
    "      for i in range(total_batches):\n",
    "          offset = (i * batch_size)\n",
    "          batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "          batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "          feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "          _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "      test_acc = session.run(accuracy_op, \\\n",
    "                             feed_dict ={tf_train_dataset:test_dataset,tf_train_labels: test_labels})\n",
    "      print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(l), \" test accuracy: {:.3f}%\".format(test_acc*100))\n",
    "  print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (nn.max_pool()) of stride 2 and kernel size 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The bar is much higher now with 95.47% accuracy obtained from last model. In this problem we will use our last section model/functions and chnage the stride together with introduction of max-pool layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "batch_size = 16\n",
    "filter_size = 5\n",
    "in_image_channnels = 1 #grayscale image\n",
    "depth1 = 16 #no of channels in conv1 layer\n",
    "depth2 = 16 #no of channels in conv2 layer\n",
    "num_hidden = 64\n",
    "learning_rate = 0.05\n",
    "\n",
    "\"\"\"\n",
    "tensorflow conv2d requires 4D input and filter tensor;\n",
    "tf.nn.conv2d(\n",
    "    input,\n",
    "    filter,\n",
    "    strides,\n",
    "    padding,...)\n",
    "input tensor of shape = [batch, in_height, in_width, in_channels]\n",
    "filter / kernel tensor of shape = [filter_height, filter_width, in_channels, out_channels]\n",
    "\"\"\"\n",
    "\n",
    "#define a function that creates convolutional layer\n",
    "def create_conv_layer(in_data, num_in_channels, num_out_channels, filter_shape,conv_stride, name):\n",
    "    ''' in_data = input Data (need 4D shape defined above)\n",
    "        in_channels = no of channels in input image, 1(grayscale), 3(RGB)\n",
    "        out_channels = depth of conv layers\n",
    "        filter_shape = filter shape used for convolution, for e.g. [3,3], or [5,5]\n",
    "        name = any valid string'''\n",
    "    #define 4D shape for filter/kernel tensor that will be used for creating weights\n",
    "    conv_filter_shape = [filter_shape[0], filter_shape[1], num_in_channels, num_out_channels]\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal(conv_filter_shape, stddev = 0.1), name = name+ '_W')\n",
    "    bias = tf.Variable(tf.zeros(num_out_channels), name = name+\"_b\")\n",
    "    \n",
    "    strides = [1, conv_stride[0], conv_stride[1], 1]\n",
    "    #define the conv2d layer\n",
    "    outlayer = tf.nn.conv2d(in_data, weights, strides = strides, padding = 'SAME')\n",
    "    #add bias\n",
    "    outlayer = outlayer + bias\n",
    "    #apply ReLu activation\n",
    "    outlayer = tf.nn.relu(outlayer)\n",
    "    \n",
    "    ## now perform max pooling\n",
    "    # define the 4D dimension of pooling filter =[1, pool_filter_x, pool_filter_y, 1]\n",
    "    ksize = [1, 2, 2, 1]\n",
    "    # now define stride for pool-layer =[1, x-strides, y-strides, 1]\n",
    "    pool_stride  = [1, 2, 2, 1]\n",
    "    outlayer = tf.nn.max_pool( outlayer, ksize, strides = pool_stride, padding ='SAME')\n",
    "    return outlayer\n",
    "\n",
    "# the dataset train_dataset, train_labels is already formatted\n",
    "# Input data placeholders\n",
    "# using shape = [None, ] allows us to use it as a general label placeholder\n",
    "# shape=(batch_size, image_size, image_size, in_image_channnels)\n",
    "tf_train_dataset = tf.placeholder(tf.float32, shape=(None, image_size, image_size, in_image_channnels))\n",
    "#tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "tf_valid_dataset = tf.constant(valid_dataset)\n",
    "tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "# MODEL definition\n",
    "# now define the two conv layer\n",
    "layer1 = create_conv_layer(tf_train_dataset, in_image_channnels, depth1,\\\n",
    "                           filter_shape =[5,5],conv_stride =[1,1], name ='layer1')\n",
    "layer2 = create_conv_layer(layer1,depth1, depth2, [5,5], [1,1], name ='layer2' )\n",
    "\n",
    "# now we need to flatten and add two FC layers\n",
    "# after two layers with max-pool of stride =2, we go from 28 x 28, to 14 x 14 to 7 x 7 x,y co-ordinates, \n",
    "# but with 16 output channels.  To create the fully connected,\n",
    "# \"dense\" layer, the new shape needs to be [-1, 7 x 7 x 16]\n",
    "#flattened = tf.reshape(layer2, [-1, 7 * 7 * 16])\n",
    "flattened = tf.reshape(layer2, [-1, 7 * 7 * depth2])\n",
    "\n",
    "#setup weights and bias for dense layer\n",
    "w1 = tf.Variable(tf.truncated_normal([7*7* depth2, num_hidden], stddev = 0.1), name ='w_dense1')\n",
    "b1 = tf.Variable(tf.zeros([num_hidden]), name ='b_dense1')\n",
    "# fully connected layer1\n",
    "dense_layer1 = tf.nn.relu(tf.matmul(flattened, w1) + b1)\n",
    "\n",
    "# weights and bias for dense layer2\n",
    "w2 = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev = 0.1), name ='w_dense2')\n",
    "b2 = tf.Variable(tf.zeros([num_labels]), name = 'b_dense2')\n",
    "# define the output logits = dense_layer\n",
    "logits = tf.matmul(dense_layer1, w2) + b2\n",
    "\n",
    "# get the logits from the model\n",
    "#logits = dense_layer2\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "# Optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.nn.softmax(logits)\n",
    "# define an accuracy assessment operation\n",
    "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(tf_train_labels, 1))\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Predictions for the training, validation, and test data.\n",
    "# these predictions are derrived by \n",
    "# calling the accuracy_op operation and feeding the placeholders in feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      " Total training dataset length:  200000\n",
      " total no of batches:  12500\n"
     ]
    }
   ],
   "source": [
    "# now training part\n",
    "epochs = 1\n",
    "\n",
    "with tf.Session() as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  total_batches = int(train_labels.shape[0]/batch_size)\n",
    "  #total_batches = 1001 # = num_Steps\n",
    "  print(\" Total training dataset length: \", len(train_labels))\n",
    "  print(\" total no of batches: \", total_batches)\n",
    "  for epoch in range(epochs):\n",
    "      for i in range(total_batches):\n",
    "          offset = (i * batch_size)\n",
    "          batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "          batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "          feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "          _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "      test_acc = session.run(accuracy_op, \\\n",
    "                             feed_dict ={tf_train_dataset:test_dataset,tf_train_labels: test_labels})\n",
    "      #valid_acc = session.run(accuracy_op, feed_dict ={tf_train_dataset:valid_dataset, tf_train_labels: valid_labels})\n",
    "      print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(l), \" test accuracy: {:.2f}%\".format(test_acc*100))\n",
    "  print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMP: Error while training\n",
    "- \"An error ocurred while starting the kernel  tensorflow/core/platform/cpu_feature_guard.cc:137]\"\n",
    " If you received the message above, chances are the Jupyter or the python IDE ran out of memory.\n",
    " \n",
    " Solutions:\n",
    " - try runnning the python script from command line\n",
    " - try deleting the unused variables\n",
    " - isolate the problem\n",
    "   - in my case the error was occuring while calculating test_accuracy operation\n",
    "   - Solution that worked for me: calculate the test arracy for 1st part and 2nd part separately\n",
    " - [Last resort]: update your tensorflow or reinstall tensorflwo with dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      " Total training dataset length:  200000\n",
      " total no of batches:  12500\n",
      "At Step = 2000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 4000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 6000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 8000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 10000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 12000, Minibatch train_accuracy = 0.875000 \n",
      "Epoch: 1 cost = 0.511  test accuracy: 95.05%\n",
      "At Step = 14000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 16000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 18000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 20000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 22000, Minibatch train_accuracy = 0.687500 \n",
      "At Step = 24000, Minibatch train_accuracy = 0.937500 \n",
      "Epoch: 2 cost = 0.530  test accuracy: 95.76%\n",
      "At Step = 26000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 28000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 30000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 32000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 34000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 36000, Minibatch train_accuracy = 1.000000 \n",
      "Epoch: 3 cost = 0.543  test accuracy: 95.95%\n",
      "At Step = 38000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 40000, Minibatch train_accuracy = 0.875000 \n",
      "At Step = 42000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 44000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 46000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 48000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 50000, Minibatch train_accuracy = 0.875000 \n",
      "Epoch: 4 cost = 0.533  test accuracy: 96.11%\n",
      "At Step = 52000, Minibatch train_accuracy = 0.812500 \n",
      "At Step = 54000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 56000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 58000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 60000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 62000, Minibatch train_accuracy = 1.000000 \n",
      "Epoch: 5 cost = 0.465  test accuracy: 96.13%\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# now training part\n",
    "epochs = 5\n",
    "global_step = 0\n",
    "with tf.Session() as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  total_batches = int(train_labels.shape[0]/batch_size)\n",
    "  #total_batches = 1001 # = num_Steps\n",
    "  print(\" Total training dataset length: \", len(train_labels))\n",
    "  print(\" total no of batches: \", total_batches)\n",
    "  for epoch in range(epochs):\n",
    "      for i in range(total_batches):\n",
    "          offset = (i * batch_size)\n",
    "          batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "          batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "          feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "          _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "          global_step +=1\n",
    "          if (global_step% 2000 == 0):\n",
    "              print(\"At Step = %d, Minibatch train_accuracy = %f \" \\\n",
    "                    %(global_step,session.run(accuracy_op, feed_dict = feed_dict)))\n",
    "          del batch_data, batch_labels, feed_dict\n",
    "      test_len = int(len(test_labels)/2) # only taking half length\n",
    "      test_acc1 = session.run(accuracy_op, \\\n",
    "                             feed_dict ={tf_train_dataset:test_dataset[0:test_len, :, :, :],tf_train_labels: test_labels[0:test_len, :]})\n",
    "      test_acc2 = session.run(accuracy_op, \\\n",
    "                             feed_dict ={tf_train_dataset:test_dataset[test_len:, :, :, :],tf_train_labels: test_labels[test_len:, :]})\n",
    "      # find the avg test acuuracy from the accuracy of 1st half and 2nd half\n",
    "      test_acc = (test_acc1 + test_acc2) /2.0\n",
    "      #valid_acc = session.run(accuracy_op, feed_dict ={tf_train_dataset:valid_dataset, tf_train_labels: valid_labels})\n",
    "      print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(l), \" test accuracy: {:.2f}%\".format(test_acc*100))\n",
    "  print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Well, that an improvement from 95.47% to 96.13%.\n",
    " - we will later realize that improving accuracy above 95% will be lot tougher and in small increments\n",
    " - Also, now the training time is much longer\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic LeNet5 architecture, adding Dropout, and/or adding learning rate decay.\n",
    "- Here is the description of LeNet architecture\n",
    "\n",
    "    * Layer C1 is a convolution layer with 6 feature maps and a 5×5 kernel for each feature map.\n",
    "    * Layer S1 is a subsampling layer with 6 feature maps and a 2×2 kernel for each feature map.\n",
    "    * Layer C3 is a convolution layer with 16 feature maps and a 6×6 kernel for each feature map.\n",
    "    * Layer S4 is a subsampling layer with 16 feature maps and a 2×2 kernel for each feature map.\n",
    "    * Layer C5 is a convolution layer with 120 feature maps and a 6×6 kernel for each feature map.\n",
    "    * Layer C6 is a fully connected layer with 84 layers\n",
    "- The current network is much deeper( 16 feature maps in both C1 and C2). Not sure if the Lenet architecture can improve much of accuracy. However, practising to create the Lenet architecture will be helpful\n",
    "- Adding dropout, learning rate decay or even using AdamOptimizer seems to be  agood option.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LeNet architecture\n",
    "image_size = 28\n",
    "batch_size = 16\n",
    "filter_size = 5# we will use same filtersize for c1, c3 and c5\n",
    "in_image_channnels = 1 #grayscale image\n",
    "depth1 = 6 #no of channels in conv1 layer\n",
    "depth2 = 16 #no of channels in conv2 layer\n",
    "depth3 = 120\n",
    "num_hidden = 84\n",
    "learning_rate = 0.05\n",
    "\n",
    "\"\"\"\n",
    "tensorflow conv2d requires 4D input and filter tensor;\n",
    "tf.nn.conv2d(\n",
    "    input,\n",
    "    filter,\n",
    "    strides,\n",
    "    padding,...)\n",
    "input tensor of shape = [batch, in_height, in_width, in_channels]\n",
    "filter / kernel tensor of shape = [filter_height, filter_width, in_channels, out_channels]\n",
    "\"\"\"\n",
    "\n",
    "#define a function that creates convolutional layer\n",
    "def create_conv_layer(in_data, num_in_channels, num_out_channels, filter_shape,conv_stride, name, use_subsample= 1):\n",
    "    ''' in_data = input Data (need 4D shape defined above)\n",
    "        in_channels = no of channels in input image, 1(grayscale), 3(RGB)\n",
    "        out_channels = depth of conv layers\n",
    "        filter_shape = filter shape used for convolution, for e.g. [3,3], or [5,5]\n",
    "        name = any valid string'''\n",
    "    #define 4D shape for filter/kernel tensor that will be used for creating weights\n",
    "    conv_filter_shape = [filter_shape[0], filter_shape[1], num_in_channels, num_out_channels]\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal(conv_filter_shape, stddev = 0.1), name = name+ '_W')\n",
    "    bias = tf.Variable(tf.zeros(num_out_channels), name = name+\"_b\")\n",
    "    \n",
    "    strides = [1, conv_stride[0], conv_stride[1], 1]\n",
    "    #define the conv2d layer\n",
    "    outlayer = tf.nn.conv2d(in_data, weights, strides = strides, padding = 'SAME')\n",
    "    #add bias\n",
    "    outlayer = outlayer + bias\n",
    "    #apply ReLu activation\n",
    "    outlayer = tf.nn.relu(outlayer)\n",
    "    \n",
    "    ## now perform max pooling if \"use_subsample=true\"\n",
    "    if(use_subsample == 1):\n",
    "        # define the 4D dimension of pooling filter =[1, pool_filter_x, pool_filter_y, 1]\n",
    "        ksize = [1, 2, 2, 1]\n",
    "        # now define stride for pool-layer =[1, x-strides, y-strides, 1]\n",
    "        pool_stride  = [1, 2, 2, 1]\n",
    "        outlayer = tf.nn.max_pool( outlayer, ksize, strides = pool_stride, padding ='SAME')\n",
    "    return outlayer\n",
    "\n",
    "# the dataset train_dataset, train_labels is already formatted\n",
    "# Input data placeholders\n",
    "# using shape = [None, ] allows us to use it as a general label placeholder\n",
    "# shape=(batch_size, image_size, image_size, in_image_channnels)\n",
    "tf_train_dataset = tf.placeholder(tf.float32, shape=(None, image_size, image_size, in_image_channnels))\n",
    "#tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "tf_valid_dataset = tf.constant(valid_dataset)\n",
    "tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "# MODEL definition\n",
    "# now define the two conv layer\n",
    "layer1 = create_conv_layer(tf_train_dataset, in_image_channnels, depth1,\\\n",
    "                           filter_shape =[5,5],conv_stride =[1,1], name ='layer1')\n",
    "layer2 = create_conv_layer(layer1,depth1, depth2, [5,5], [1,1], name ='layer2' )\n",
    "layer3 = create_conv_layer(layer2,depth2, depth3, [5,5], [1,1], name ='layer3', use_subsample=0)\n",
    "\n",
    "# now we need to flatten and add two FC layers\n",
    "# after two layers with max-pool of stride =2, we go from (28,28)->(14,14)->(7,7)  \n",
    "# but with 120(depth3) output channels.  To create the fully connected,\n",
    "# \"dense\" layer, the new shape needs to be [-1, 7 x 7 x 120]\n",
    "flattened = tf.reshape(layer3, [-1, 7 * 7 * depth3])\n",
    "\n",
    "#setup weights and bias for dense layer\n",
    "w1 = tf.Variable(tf.truncated_normal([7*7* depth3, num_hidden], stddev = 0.1), name ='w_dense1')\n",
    "b1 = tf.Variable(tf.zeros([num_hidden]), name ='b_dense1')\n",
    "# fully connected layer1\n",
    "dense_layer1 = tf.nn.relu(tf.matmul(flattened, w1) + b1)\n",
    "\n",
    "# weights and bias for dense layer2\n",
    "w2 = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev = 0.1), name ='w_dense2')\n",
    "b2 = tf.Variable(tf.zeros([num_labels]), name = 'b_dense2')\n",
    "# define the output logits = dense_layer\n",
    "logits = tf.matmul(dense_layer1, w2) + b2\n",
    "\n",
    "# get the logits from the model\n",
    "#logits = dense_layer2\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "# Optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.nn.softmax(logits)\n",
    "# define an accuracy assessment operation\n",
    "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(tf_train_labels, 1))\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Predictions for the training, validation, and test data.\n",
    "# these predictions are derrived by \n",
    "# calling the accuracy_op operation and feeding the placeholders in feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      " Total training dataset length:  200000\n",
      " total no of batches:  12500\n",
      "At Step = 2000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 4000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 6000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 8000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 10000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 12000, Minibatch train_accuracy = 0.937500 \n",
      "Epoch: 1 cost = 0.499  test accuracy: 95.44%\n",
      "At Step = 14000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 16000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 18000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 20000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 22000, Minibatch train_accuracy = 0.812500 \n",
      "At Step = 24000, Minibatch train_accuracy = 0.937500 \n",
      "Epoch: 2 cost = 0.424  test accuracy: 95.83%\n",
      "At Step = 26000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 28000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 30000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 32000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 34000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 36000, Minibatch train_accuracy = 1.000000 \n",
      "Epoch: 3 cost = 0.573  test accuracy: 95.74%\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# now training part\n",
    "epochs = 3\n",
    "step_counter = 0\n",
    "with tf.Session() as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  total_batches = int(train_labels.shape[0]/batch_size)\n",
    "  #total_batches = 1001 # = num_Steps\n",
    "  print(\" Total training dataset length: \", len(train_labels))\n",
    "  print(\" total no of batches: \", total_batches)\n",
    "  for epoch in range(epochs):\n",
    "      for i in range(total_batches):\n",
    "          offset = (i * batch_size)\n",
    "          batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "          batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "          feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "          _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "          step_counter +=1\n",
    "          if (step_counter% 2000 == 0):\n",
    "              print(\"At Step = %d, Minibatch train_accuracy = %f \" \\\n",
    "                    %(step_counter,session.run(accuracy_op, feed_dict = feed_dict)))\n",
    "          del batch_data, batch_labels, feed_dict\n",
    "      test_len = int(len(test_labels)/2) # only taking half length\n",
    "      test_acc1 = session.run(accuracy_op, \\\n",
    "                             feed_dict ={tf_train_dataset:test_dataset[0:test_len, :, :, :],tf_train_labels: test_labels[0:test_len, :]})\n",
    "      test_acc2 = session.run(accuracy_op, \\\n",
    "                             feed_dict ={tf_train_dataset:test_dataset[test_len:, :, :, :],tf_train_labels: test_labels[test_len:, :]})\n",
    "      # find the avg test acuuracy from the accuracy of 1st half and 2nd half\n",
    "      test_acc = (test_acc1 + test_acc2) /2.0\n",
    "      #valid_acc = session.run(accuracy_op, feed_dict ={tf_train_dataset:valid_dataset, tf_train_labels: valid_labels})\n",
    "      print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(l), \" test accuracy: {:.2f}%\".format(test_acc*100))\n",
    "  print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is almost the same accuracy as the problem1(after the end of epoch3).\n",
    "\n",
    "Next we will add, learning rate decay and Adam optimer to examine the effect of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LeNet architecture\n",
    "image_size = 28\n",
    "batch_size = 16\n",
    "filter_size = 5# we will use same filtersize for c1, c3 and c5\n",
    "in_image_channnels = 1 #grayscale image\n",
    "depth1 = 6 #no of channels in conv1 layer\n",
    "depth2 = 16 #no of channels in conv2 layer\n",
    "depth3 = 120\n",
    "num_hidden = 84\n",
    "learning_rate = 0.05\n",
    "global_step = tf.Variable(0)\n",
    "\n",
    "\"\"\"\n",
    "tensorflow conv2d requires 4D input and filter tensor;\n",
    "tf.nn.conv2d(\n",
    "    input,\n",
    "    filter,\n",
    "    strides,\n",
    "    padding,...)\n",
    "input tensor of shape = [batch, in_height, in_width, in_channels]\n",
    "filter / kernel tensor of shape = [filter_height, filter_width, in_channels, out_channels]\n",
    "\"\"\"\n",
    "\n",
    "#define a function that creates convolutional layer\n",
    "def create_conv_layer(in_data, num_in_channels, num_out_channels, filter_shape,conv_stride, name, use_subsample= 1):\n",
    "    ''' in_data = input Data (need 4D shape defined above)\n",
    "        in_channels = no of channels in input image, 1(grayscale), 3(RGB)\n",
    "        out_channels = depth of conv layers\n",
    "        filter_shape = filter shape used for convolution, for e.g. [3,3], or [5,5]\n",
    "        name = any valid string'''\n",
    "    #define 4D shape for filter/kernel tensor that will be used for creating weights\n",
    "    conv_filter_shape = [filter_shape[0], filter_shape[1], num_in_channels, num_out_channels]\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal(conv_filter_shape, stddev = 0.1), name = name+ '_W')\n",
    "    bias = tf.Variable(tf.zeros(num_out_channels), name = name+\"_b\")\n",
    "    \n",
    "    strides = [1, conv_stride[0], conv_stride[1], 1]\n",
    "    #define the conv2d layer\n",
    "    outlayer = tf.nn.conv2d(in_data, weights, strides = strides, padding = 'SAME')\n",
    "    #add bias\n",
    "    outlayer = outlayer + bias\n",
    "    #apply ReLu activation\n",
    "    outlayer = tf.nn.relu(outlayer)\n",
    "    \n",
    "    ## now perform max pooling if \"use_subsample=true\"\n",
    "    if(use_subsample == 1):\n",
    "        # define the 4D dimension of pooling filter =[1, pool_filter_x, pool_filter_y, 1]\n",
    "        ksize = [1, 2, 2, 1]\n",
    "        # now define stride for pool-layer =[1, x-strides, y-strides, 1]\n",
    "        pool_stride  = [1, 2, 2, 1]\n",
    "        outlayer = tf.nn.max_pool( outlayer, ksize, strides = pool_stride, padding ='SAME')\n",
    "    return outlayer\n",
    "\n",
    "# the dataset train_dataset, train_labels is already formatted\n",
    "# Input data placeholders\n",
    "# using shape = [None, ] allows us to use it as a general label placeholder\n",
    "# shape=(batch_size, image_size, image_size, in_image_channnels)\n",
    "tf_train_dataset = tf.placeholder(tf.float32, shape=(None, image_size, image_size, in_image_channnels))\n",
    "#tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "tf_valid_dataset = tf.constant(valid_dataset)\n",
    "tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "# MODEL definition\n",
    "# now define the two conv layer\n",
    "layer1 = create_conv_layer(tf_train_dataset, in_image_channnels, depth1,\\\n",
    "                           filter_shape =[5,5],conv_stride =[1,1], name ='layer1')\n",
    "layer2 = create_conv_layer(layer1,depth1, depth2, [5,5], [1,1], name ='layer2' )\n",
    "layer3 = create_conv_layer(layer2,depth2, depth3, [5,5], [1,1], name ='layer3', use_subsample=0)\n",
    "\n",
    "# now we need to flatten and add two FC layers\n",
    "# after two layers with max-pool of stride =2, we go from (28,28)->(14,14)->(7,7)  \n",
    "# but with 120(depth3) output channels.  To create the fully connected,\n",
    "# \"dense\" layer, the new shape needs to be [-1, 7 x 7 x 120]\n",
    "flattened = tf.reshape(layer3, [-1, 7 * 7 * depth3])\n",
    "\n",
    "#setup weights and bias for dense layer\n",
    "w1 = tf.Variable(tf.truncated_normal([7*7* depth3, num_hidden], stddev = 0.1), name ='w_dense1')\n",
    "b1 = tf.Variable(tf.zeros([num_hidden]), name ='b_dense1')\n",
    "# fully connected layer1\n",
    "dense_layer1 = tf.nn.relu(tf.matmul(flattened, w1) + b1)\n",
    "\n",
    "# weights and bias for dense layer2\n",
    "w2 = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev = 0.1), name ='w_dense2')\n",
    "b2 = tf.Variable(tf.zeros([num_labels]), name = 'b_dense2')\n",
    "# define the output logits = dense_layer\n",
    "logits = tf.matmul(dense_layer1, w2) + b2\n",
    "\n",
    "# get the logits from the model\n",
    "#logits = dense_layer2\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "# Optimizer.\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "#global_step = tf.Variable(0)\n",
    "#learning_rate = tf.train.exponential_decay(0.5, global_step,decay_steps=1000, decay_rate= 0.65, staircase=True)\n",
    "#learning_rate = 0.05\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "\n",
    "prediction = tf.nn.softmax(logits)\n",
    "# define an accuracy assessment operation\n",
    "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(tf_train_labels, 1))\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Predictions for the training, validation, and test data.\n",
    "# these predictions are derrived by \n",
    "# calling the accuracy_op operation and feeding the placeholders in feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      " Total training dataset length:  200000\n",
      " total no of batches:  12500\n",
      "At Step = 2000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 4000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 6000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 8000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 10000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 12000, Minibatch train_accuracy = 0.875000 \n",
      "Epoch: 1 cost = 0.776  test accuracy: 95.35%\n",
      "At Step = 14000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 16000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 18000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 20000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 22000, Minibatch train_accuracy = 0.812500 \n",
      "At Step = 24000, Minibatch train_accuracy = 0.937500 \n",
      "Epoch: 2 cost = 0.562  test accuracy: 95.78%\n",
      "At Step = 26000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 28000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 30000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 32000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 34000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 36000, Minibatch train_accuracy = 1.000000 \n",
      "Epoch: 3 cost = 0.557  test accuracy: 95.93%\n",
      "At Step = 38000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 40000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 42000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 44000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 46000, Minibatch train_accuracy = 0.875000 \n",
      "At Step = 48000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 50000, Minibatch train_accuracy = 0.937500 \n",
      "Epoch: 4 cost = 0.475  test accuracy: 96.20%\n",
      "At Step = 52000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 54000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 56000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 58000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 60000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 62000, Minibatch train_accuracy = 1.000000 \n",
      "Epoch: 5 cost = 0.466  test accuracy: 96.01%\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# now training part\n",
    "epochs = 5\n",
    "step_counter = 0\n",
    "with tf.Session() as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  total_batches = int(train_labels.shape[0]/batch_size)\n",
    "  #total_batches = 1001 # = num_Steps\n",
    "  print(\" Total training dataset length: \", len(train_labels))\n",
    "  print(\" total no of batches: \", total_batches)\n",
    "  for epoch in range(epochs):\n",
    "      for i in range(total_batches):\n",
    "          offset = (i * batch_size)\n",
    "          batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "          batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "          feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "          _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "          step_counter +=1\n",
    "          if (step_counter% 2000 == 0):\n",
    "              print(\"At Step = %d, Minibatch train_accuracy = %f \" \\\n",
    "                    %(step_counter,session.run(accuracy_op, feed_dict = feed_dict)))\n",
    "          del batch_data, batch_labels, feed_dict\n",
    "      test_len = int(len(test_labels)/2) # only taking half length\n",
    "      test_acc1 = session.run(accuracy_op, \\\n",
    "                             feed_dict ={tf_train_dataset:test_dataset[0:test_len, :, :, :],tf_train_labels: test_labels[0:test_len, :]})\n",
    "      test_acc2 = session.run(accuracy_op, \\\n",
    "                             feed_dict ={tf_train_dataset:test_dataset[test_len:, :, :, :],tf_train_labels: test_labels[test_len:, :]})\n",
    "      # find the avg test acuuracy from the accuracy of 1st half and 2nd half\n",
    "      test_acc = (test_acc1 + test_acc2) /2.0\n",
    "      #valid_acc = session.run(accuracy_op, feed_dict ={tf_train_dataset:valid_dataset, tf_train_labels: valid_labels})\n",
    "      print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(l), \" test accuracy: {:.2f}%\".format(test_acc*100))\n",
    "  print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
