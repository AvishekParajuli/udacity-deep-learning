{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized CNN for notMNIST digit classificatin\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will build a network which is deeper than the  one in  and LeNet5.\n",
    "\n",
    "The achitecture is:\n",
    "Input(N,28, 28,1)->{conv(5,5,32, s=1)+ReLu+MaxPool(2,2)}->{conv(5,5,64, s=2)+ReLu+MaxPool(2,2)}->FC->FC(1000)->Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Test set (10000, 28, 28) (10000,)\n",
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "#from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  #valid_dataset = save['valid_dataset']\n",
    "  #valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  #print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "#%%\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "#valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "#print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "batch_size = 16\n",
    "filter_size = 5\n",
    "in_image_channnels = 1 #grayscale image\n",
    "depth1 = 32 #no of channels in conv1 layer\n",
    "depth2 = 64 #no of channels in conv2 layer\n",
    "num_hidden = 1000\n",
    "learning_rate = 0.05\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \"\"\"\n",
    "    tensorflow conv2d requires 4D input and filter tensor;\n",
    "    tf.nn.conv2d(\n",
    "        input,\n",
    "        filter,\n",
    "        strides,\n",
    "        padding,...)\n",
    "    input tensor of shape = [batch, in_height, in_width, in_channels]\n",
    "    filter / kernel tensor of shape = [filter_height, filter_width, in_channels, out_channels]\n",
    "    \"\"\"\n",
    "    \n",
    "    #define a function that creates convolutional layer\n",
    "    def create_conv_layer(in_data, num_in_channels, num_out_channels, filter_shape,conv_stride, name, use_subsample= 1):\n",
    "        ''' in_data = input Data (need 4D shape defined above)\n",
    "            in_channels = no of channels in input image, 1(grayscale), 3(RGB)\n",
    "            out_channels = depth of conv layers\n",
    "            filter_shape = filter shape used for convolution, for e.g. [3,3], or [5,5]\n",
    "            name = any valid string'''\n",
    "        #define 4D shape for filter/kernel tensor that will be used for creating weights\n",
    "        conv_filter_shape = [filter_shape[0], filter_shape[1], num_in_channels, num_out_channels]\n",
    "        \n",
    "        weights = tf.Variable(tf.truncated_normal(conv_filter_shape, stddev = 0.1), name = name+ '_W')\n",
    "        bias = tf.Variable(tf.zeros(num_out_channels), name = name+\"_b\")\n",
    "        \n",
    "        strides = [1, conv_stride[0], conv_stride[1], 1]\n",
    "        #define the conv2d layer\n",
    "        outlayer = tf.nn.conv2d(in_data, weights, strides = strides, padding = 'SAME')\n",
    "        #add bias\n",
    "        outlayer = outlayer + bias\n",
    "        #apply ReLu activation\n",
    "        outlayer = tf.nn.relu(outlayer)\n",
    "        \n",
    "        if(use_subsample== 1):\n",
    "            ## now perform max pooling\n",
    "            # define the 4D dimension of pooling filter =[1, pool_filter_x, pool_filter_y, 1]\n",
    "            ksize = [1, 2, 2, 1]\n",
    "            # now define stride for pool-layer =[1, x-strides, y-strides, 1]\n",
    "            pool_stride  = [1, 2, 2, 1]\n",
    "            outlayer = tf.nn.max_pool( outlayer, ksize, strides = pool_stride, padding ='SAME')\n",
    "        return outlayer\n",
    "    \n",
    "    # the dataset train_dataset, train_labels is already formatted\n",
    "    # Input data placeholders\n",
    "    # using shape = [None, ] allows us to use it as a general label placeholder\n",
    "    # shape=(batch_size, image_size, image_size, in_image_channnels)\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(None, image_size, image_size, in_image_channnels))\n",
    "    #tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "    #tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    #tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # MODEL definition\n",
    "    # now define the two conv layer\n",
    "    layer1 = create_conv_layer(tf_train_dataset, in_image_channnels, depth1,\\\n",
    "                               filter_shape =[5,5],conv_stride =[1,1], name ='layer1')\n",
    "    layer2 = create_conv_layer(layer1,depth1, depth2, [5,5], [1,1], name ='layer2' )\n",
    "    \n",
    "    # now we need to flatten and add two FC layers\n",
    "    # after two layers with max-pool of stride =2, we go from 28 x 28, to 14 x 14 to 7 x 7 x,y co-ordinates, \n",
    "    # but with 16 output channels.  To create the fully connected,\n",
    "    # \"dense\" layer, the new shape needs to be [-1, 7 x 7 x 16]\n",
    "    #flattened = tf.reshape(layer2, [-1, 7 * 7 * 16])\n",
    "    flattened = tf.reshape(layer2, [-1, 7 * 7 * depth2])\n",
    "    \n",
    "    #setup weights and bias for dense layer\n",
    "    w1 = tf.Variable(tf.truncated_normal([7*7* depth2, num_hidden], stddev = 0.1), name ='w_dense1')\n",
    "    b1 = tf.Variable(tf.zeros([num_hidden]), name ='b_dense1')\n",
    "    # fully connected layer1\n",
    "    dense_layer1 = tf.nn.relu(tf.matmul(flattened, w1) + b1)\n",
    "    \n",
    "    # weights and bias for dense layer2\n",
    "    w2 = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev = 0.1), name ='w_dense2')\n",
    "    b2 = tf.Variable(tf.zeros([num_labels]), name = 'b_dense2')\n",
    "    # define the output logits = dense_layer2\n",
    "    logits = tf.matmul(dense_layer1, w2) + b2\n",
    "    \n",
    "    # get the logits from the model\n",
    "    #logits = dense_layer2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits))\n",
    "        \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    prediction = tf.nn.softmax(logits)\n",
    "    # define an accuracy assessment operation\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(tf_train_labels, 1))\n",
    "    accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#end of model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      " Total training dataset length:  200000\n",
      " total no of batches:  12500\n",
      "At Step = 2000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 4000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 6000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 8000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 10000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 12000, Minibatch train_accuracy = 1.000000 \n",
      "Epoch: 1 cost = 0.621  test accuracy: 95.66%\n",
      "At Step = 14000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 16000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 18000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 20000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 22000, Minibatch train_accuracy = 0.875000 \n",
      "At Step = 24000, Minibatch train_accuracy = 1.000000 \n",
      "Epoch: 2 cost = 0.515  test accuracy: 96.12%\n",
      "At Step = 26000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 28000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 30000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 32000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 34000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 36000, Minibatch train_accuracy = 1.000000 \n",
      "Epoch: 3 cost = 0.438  test accuracy: 96.02%\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# now training part\n",
    "epochs = 3\n",
    "global_step = 0\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  total_batches = int(train_labels.shape[0]/batch_size)\n",
    "  #total_batches = 1001 # = num_Steps\n",
    "  print(\" Total training dataset length: \", len(train_labels))\n",
    "  print(\" total no of batches: \", total_batches)\n",
    "  for epoch in range(epochs):\n",
    "      for i in range(total_batches):\n",
    "          offset = (i * batch_size)\n",
    "          batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "          batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "          feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "          _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "          global_step +=1\n",
    "          if (global_step% 2000 == 0):\n",
    "              print(\"At Step = %d, Minibatch train_accuracy = %f \" \\\n",
    "                    %(global_step,session.run(accuracy_op, feed_dict = feed_dict)))\n",
    "          del batch_data, batch_labels, feed_dict\n",
    "      test_len = int(len(test_labels)/2) # only taking half length\n",
    "      test_acc1 = session.run(accuracy_op, \\\n",
    "                             feed_dict ={tf_train_dataset:test_dataset[0:test_len, :, :, :],tf_train_labels: test_labels[0:test_len, :]})\n",
    "      test_acc2 = session.run(accuracy_op, \\\n",
    "                             feed_dict ={tf_train_dataset:test_dataset[test_len:, :, :, :],tf_train_labels: test_labels[test_len:, :]})\n",
    "      # find the avg test acuuracy from the accuracy of 1st half and 2nd half\n",
    "      test_acc = (test_acc1 + test_acc2) /2.0\n",
    "      print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(l), \" test accuracy: {:.2f}%\".format(test_acc*100))\n",
    "  print(\"\\nTraining complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a fairly big CNN compared to the exercises in 4_convolution.ipynb.\n",
    "\n",
    "As we see, that after epoch 1, training accuracy has been 100% but the test accuracy is still 96%. This is clearly overfitting.\n",
    "\n",
    "Next we will use dropout layer and l2 regularization to improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "batch_size = 16\n",
    "filter_size = 5\n",
    "in_image_channnels = 1 #grayscale image\n",
    "depth1 = 16 #no of channels in conv1 layer\n",
    "depth2 = 32 #no of channels in conv2 layer\n",
    "num_hidden = 100\n",
    "learning_rate = 0.05\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \"\"\"\n",
    "    tensorflow conv2d requires 4D input and filter tensor;\n",
    "    tf.nn.conv2d(\n",
    "        input,\n",
    "        filter,\n",
    "        strides,\n",
    "        padding,...)\n",
    "    input tensor of shape = [batch, in_height, in_width, in_channels]\n",
    "    filter / kernel tensor of shape = [filter_height, filter_width, in_channels, out_channels]\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #define a function that creates convolutional layer\n",
    "    def create_conv_layer(in_data, num_in_channels, num_out_channels, filter_shape,conv_stride, name, use_subsample= 1):\n",
    "        ''' in_data = input Data (need 4D shape defined above)\n",
    "            in_channels = no of channels in input image, 1(grayscale), 3(RGB)\n",
    "            out_channels = depth of conv layers\n",
    "            filter_shape = filter shape used for convolution, for e.g. [3,3], or [5,5]\n",
    "            name = any valid string'''\n",
    "        #define 4D shape for filter/kernel tensor that will be used for creating weights\n",
    "        conv_filter_shape = [filter_shape[0], filter_shape[1], num_in_channels, num_out_channels]\n",
    "        \n",
    "        weights = tf.Variable(tf.truncated_normal(conv_filter_shape, stddev = 0.1), name = name+ '_W')\n",
    "        bias = tf.Variable(tf.zeros(num_out_channels), name = name+\"_b\")\n",
    "        \n",
    "        strides = [1, conv_stride[0], conv_stride[1], 1]\n",
    "        #define the conv2d layer\n",
    "        outlayer = tf.nn.conv2d(in_data, weights, strides = strides, padding = 'SAME')\n",
    "        #add bias\n",
    "        outlayer = outlayer + bias\n",
    "        #apply ReLu activation\n",
    "        outlayer = tf.nn.relu(outlayer)\n",
    "        # add dropout layer to model; in training keep_prob=0.5, in test keep_prob =1\n",
    "        #outlayer = tf.nn.dropout(outlayer, keep_prob = keep_prob)\n",
    "        \n",
    "        if(use_subsample== 1):\n",
    "            ## now perform max pooling\n",
    "            # define the 4D dimension of pooling filter =[1, pool_filter_x, pool_filter_y, 1]\n",
    "            ksize = [1, 2, 2, 1]\n",
    "            # now define stride for pool-layer =[1, x-strides, y-strides, 1]\n",
    "            pool_stride  = [1, 2, 2, 1]\n",
    "            outlayer = tf.nn.max_pool( outlayer, ksize, strides = pool_stride, padding ='SAME')\n",
    "        return outlayer\n",
    "    \n",
    "    # the dataset train_dataset, train_labels is already formatted\n",
    "    # Input data placeholders\n",
    "    # using shape = [None, ] allows us to use it as a general label placeholder\n",
    "    # shape=(batch_size, image_size, image_size, in_image_channnels)\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(None, image_size, image_size, in_image_channnels))\n",
    "    #tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "    #tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    #tf_test_dataset = tf.constant(test_dataset)\n",
    "    #keep_prob = tf.placeholder(tf.float32)\n",
    "    beta = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # MODEL definition\n",
    "    # now define the two conv layer\n",
    "    layer1 = create_conv_layer(tf_train_dataset, in_image_channnels, depth1,\\\n",
    "                               filter_shape =[5,5],conv_stride =[1,1], name ='layer1')\n",
    "    layer2 = create_conv_layer(layer1,depth1, depth2, [5,5], [1,1], name ='layer2' )\n",
    "    \n",
    "    # now we need to flatten and add two FC layers\n",
    "    # after two layers with max-pool of stride =2, we go from 28 x 28, to 14 x 14 to 7 x 7 x,y co-ordinates, \n",
    "    # but with 16 output channels.  To create the fully connected,\n",
    "    # \"dense\" layer, the new shape needs to be [-1, 7 x 7 x 16]\n",
    "    #flattened = tf.reshape(layer2, [-1, 7 * 7 * 16])\n",
    "    flattened = tf.reshape(layer2, [-1, 7 * 7 * depth2])\n",
    "    \n",
    "    #setup weights and bias for dense layer\n",
    "    w1 = tf.Variable(tf.truncated_normal([7*7* depth2, num_hidden], stddev = 0.1), name ='w_dense1')\n",
    "    b1 = tf.Variable(tf.zeros([num_hidden]), name ='b_dense1')\n",
    "    # fully connected layer1\n",
    "    dense_layer1 = tf.nn.relu(tf.matmul(flattened, w1) + b1)\n",
    "    \n",
    "    # weights and bias for dense layer2\n",
    "    w2 = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev = 0.1), name ='w_dense2')\n",
    "    b2 = tf.Variable(tf.zeros([num_labels]), name = 'b_dense2')\n",
    "    # define the output logits = dense_layer2\n",
    "    logits = tf.matmul(dense_layer1, w2) + b2\n",
    "    \n",
    "    # get the logits from the model\n",
    "    #logits = dense_layer2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits)) + \\\n",
    "    beta *(tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2))\n",
    "    # note only the dense layers weights are normalized; its a bit of more work to normalize cnn weights\n",
    "        \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    prediction = tf.nn.softmax(logits)\n",
    "    # define an accuracy assessment operation\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(tf_train_labels, 1))\n",
    "    accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#end of model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      " Total training dataset length:  200000\n",
      " total no of batches:  12500\n",
      "At Step = 2000, Minibatch train_accuracy = 0.812500 \n",
      "At Step = 4000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 6000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 8000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 10000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 12000, Minibatch train_accuracy = 0.937500 \n",
      "Epoch: 1 cost = 0.882  test accuracy: 95.58%\n",
      "At Step = 14000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 16000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 18000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 20000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 22000, Minibatch train_accuracy = 0.750000 \n",
      "At Step = 24000, Minibatch train_accuracy = 0.937500 \n",
      "Epoch: 2 cost = 0.759  test accuracy: 96.33%\n",
      "At Step = 26000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 28000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 30000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 32000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 34000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 36000, Minibatch train_accuracy = 0.937500 \n",
      "Epoch: 3 cost = 0.682  test accuracy: 96.40%\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# now training part\n",
    "epochs = 3\n",
    "global_step = 0\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  total_batches = int(train_labels.shape[0]/batch_size)\n",
    "  #total_batches = 1001 # = num_Steps\n",
    "  print(\" Total training dataset length: \", len(train_labels))\n",
    "  print(\" total no of batches: \", total_batches)\n",
    "  for epoch in range(epochs):\n",
    "      for i in range(total_batches):\n",
    "          offset = (i * batch_size)\n",
    "          batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "          batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "          feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta:1e-3}\n",
    "          _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "          global_step +=1\n",
    "          if (global_step% 2000 == 0):\n",
    "              print(\"At Step = %d, Minibatch train_accuracy = %f \" \\\n",
    "                    %(global_step,session.run(accuracy_op, feed_dict = feed_dict)))\n",
    "          del batch_data, batch_labels, feed_dict\n",
    "      test_len = int(len(test_labels)/2) # only taking half length\n",
    "      test_acc1 = session.run(accuracy_op, \\\n",
    "                             feed_dict ={tf_train_dataset:test_dataset[0:test_len, :, :, :],tf_train_labels: test_labels[0:test_len, :], beta:3e-3})\n",
    "      test_acc2 = session.run(accuracy_op, \\\n",
    "                             feed_dict ={tf_train_dataset:test_dataset[test_len:, :, :, :],tf_train_labels: test_labels[test_len:, :], beta:3e-3})\n",
    "      # find the avg test acuuracy from the accuracy of 1st half and 2nd half\n",
    "      test_acc = (test_acc1 + test_acc2) /2.0\n",
    "      print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(l), \" test accuracy: {:.2f}%\".format(test_acc*100))\n",
    "  print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "96.4% is the best accuracy.\n",
    "Now we will train over 5 epochs and see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      " Total training dataset length:  200000\n",
      " total no of batches:  12500\n",
      "At Step = 2000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 4000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 6000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 8000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 10000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 12000, Minibatch train_accuracy = 0.937500 \n",
      "Epoch: 1 cost = 0.729  test accuracy: 95.45%\n",
      "At Step = 14000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 16000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 18000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 20000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 22000, Minibatch train_accuracy = 0.875000 \n",
      "At Step = 24000, Minibatch train_accuracy = 1.000000 \n",
      "Epoch: 2 cost = 0.661  test accuracy: 95.73%\n",
      "At Step = 26000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 28000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 30000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 32000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 34000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 36000, Minibatch train_accuracy = 1.000000 \n",
      "Epoch: 3 cost = 0.541  test accuracy: 95.97%\n",
      "At Step = 38000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 40000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 42000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 44000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 46000, Minibatch train_accuracy = 0.875000 \n",
      "At Step = 48000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 50000, Minibatch train_accuracy = 0.937500 \n",
      "Epoch: 4 cost = 0.594  test accuracy: 96.05%\n",
      "At Step = 52000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 54000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 56000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 58000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 60000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 62000, Minibatch train_accuracy = 1.000000 \n",
      "Epoch: 5 cost = 0.646  test accuracy: 96.35%\n",
      "At Step = 64000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 66000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 68000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 70000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 72000, Minibatch train_accuracy = 0.875000 \n",
      "At Step = 74000, Minibatch train_accuracy = 1.000000 \n",
      "Epoch: 6 cost = 0.611  test accuracy: 96.28%\n",
      "At Step = 76000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 78000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 80000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 82000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 84000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 86000, Minibatch train_accuracy = 1.000000 \n",
      "Epoch: 7 cost = 0.549  test accuracy: 96.34%\n",
      "At Step = 88000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 90000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 92000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 94000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 96000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 98000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 100000, Minibatch train_accuracy = 1.000000 \n",
      "Epoch: 8 cost = 0.752  test accuracy: 96.24%\n",
      "At Step = 102000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 104000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 106000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 108000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 110000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 112000, Minibatch train_accuracy = 1.000000 \n",
      "Epoch: 9 cost = 0.617  test accuracy: 96.12%\n",
      "At Step = 114000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 116000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 118000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 120000, Minibatch train_accuracy = 1.000000 \n",
      "At Step = 122000, Minibatch train_accuracy = 0.937500 \n",
      "At Step = 124000, Minibatch train_accuracy = 1.000000 \n",
      "Epoch: 10 cost = 0.668  test accuracy: 96.10%\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# now training part\n",
    "epochs = 10\n",
    "global_step = 0\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  total_batches = int(train_labels.shape[0]/batch_size)\n",
    "  #total_batches = 1001 # = num_Steps\n",
    "  print(\" Total training dataset length: \", len(train_labels))\n",
    "  print(\" total no of batches: \", total_batches)\n",
    "  for epoch in range(epochs):\n",
    "      for i in range(total_batches):\n",
    "          offset = (i * batch_size)\n",
    "          batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "          batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "          feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta:1e-3}\n",
    "          _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "          global_step +=1\n",
    "          if (global_step% 2000 == 0):\n",
    "              print(\"At Step = %d, Minibatch train_accuracy = %f \" \\\n",
    "                    %(global_step,session.run(accuracy_op, feed_dict = feed_dict)))\n",
    "          del batch_data, batch_labels, feed_dict\n",
    "      test_len = int(len(test_labels)/2) # only taking half length\n",
    "      test_acc1 = session.run(accuracy_op, \\\n",
    "                             feed_dict ={tf_train_dataset:test_dataset[0:test_len, :, :, :],tf_train_labels: test_labels[0:test_len, :], beta:3e-3})\n",
    "      test_acc2 = session.run(accuracy_op, \\\n",
    "                             feed_dict ={tf_train_dataset:test_dataset[test_len:, :, :, :],tf_train_labels: test_labels[test_len:, :], beta:3e-3})\n",
    "      # find the avg test acuuracy from the accuracy of 1st half and 2nd half\n",
    "      test_acc = (test_acc1 + test_acc2) /2.0\n",
    "      print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(l), \" test accuracy: {:.2f}%\".format(test_acc*100))\n",
    "  print(\"\\nTraining complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
